{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis: Sentiment Classification Model\n",
    "\n",
    "This notebook performs comprehensive error analysis on the trained sentiment analysis model.\n",
    "\n",
    "## Analysis Components:\n",
    "1. Load predictions from evaluation\n",
    "2. Identify misclassified examples\n",
    "3. Analyze error patterns\n",
    "4. Performance by text length\n",
    "5. Confidence score analysis\n",
    "6. Visualizations (confusion matrix, ROC curve)\n",
    "7. Example misclassifications\n",
    "8. Recommendations for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "import json\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "predictions_df = pd.read_csv('../evaluation_results/predictions.csv')\n",
    "\n",
    "# Load metrics\n",
    "with open('../evaluation_results/metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(f\"Total samples: {len(predictions_df)}\")\n",
    "print(f\"Correct predictions: {predictions_df['correct'].sum()}\")\n",
    "print(f\"Incorrect predictions: {(~predictions_df['correct']).sum()}\")\n",
    "print(f\"\\nOverall Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original test data to get text\n",
    "from src.sentiment_analyzer.data.data_loader import SentimentDataLoader\n",
    "\n",
    "loader = SentimentDataLoader(\n",
    "    data_path='../data/raw/amazon_polarity_20k.csv',\n",
    "    train_split=0.6,\n",
    "    test_split=0.2,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "_, _, test_dataset = loader.load_and_prepare()\n",
    "test_df = test_dataset.to_pandas()\n",
    "\n",
    "# Add text to predictions\n",
    "predictions_df['text'] = test_df['text'].values\n",
    "predictions_df['text_length'] = predictions_df['text'].str.len()\n",
    "predictions_df['word_count'] = predictions_df['text'].str.split().str.len()\n",
    "\n",
    "print(f\"Text added to {len(predictions_df)} predictions\")\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(predictions_df['true_label'], predictions_df['predicted_label'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation_results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives:  {cm[0,0]} ({cm[0,0]/cm[0].sum()*100:.1f}%)\")\n",
    "print(f\"False Positives: {cm[0,1]} ({cm[0,1]/cm[0].sum()*100:.1f}%)\")\n",
    "print(f\"False Negatives: {cm[1,0]} ({cm[1,0]/cm[1].sum()*100:.1f}%)\")\n",
    "print(f\"True Positives:  {cm[1,1]} ({cm[1,1]/cm[1].sum()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(predictions_df['true_label'], \n",
    "                                   predictions_df['prob_positive'])\n",
    "roc_auc = roc_auc_score(predictions_df['true_label'], \n",
    "                        predictions_df['prob_positive'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation_results/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis by Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define length buckets\n",
    "bins = [0, 50, 200, 500, 1000, 10000]\n",
    "labels = ['Very Short\\n(0-50)', 'Short\\n(51-200)', 'Medium\\n(201-500)', \n",
    "          'Long\\n(501-1000)', 'Very Long\\n(1000+)']\n",
    "predictions_df['length_bucket'] = pd.cut(predictions_df['text_length'], \n",
    "                                          bins=bins, labels=labels)\n",
    "\n",
    "# Calculate accuracy by length\n",
    "length_analysis = predictions_df.groupby('length_bucket').agg({\n",
    "    'correct': ['count', 'sum', 'mean'],\n",
    "    'confidence': 'mean',\n",
    "    'f1_score': lambda x: metrics['f1_score']  # placeholder\n",
    "}).round(4)\n",
    "\n",
    "length_analysis.columns = ['_'.join(col).strip() for col in length_analysis.columns.values]\n",
    "length_analysis = length_analysis.rename(columns={\n",
    "    'correct_count': 'total',\n",
    "    'correct_sum': 'correct',\n",
    "    'correct_mean': 'accuracy',\n",
    "    'confidence_mean': 'avg_confidence'\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance by Text Length:\")\n",
    "print(length_analysis)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy by length\n",
    "length_analysis['accuracy'].plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Accuracy by Text Length', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_xlabel('Text Length Category', fontsize=12)\n",
    "ax1.axhline(y=metrics['accuracy'], color='r', linestyle='--', label='Overall Accuracy')\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Sample count by length\n",
    "length_analysis['total'].plot(kind='bar', ax=ax2, color='lightcoral', edgecolor='black')\n",
    "ax2.set_title('Sample Distribution by Text Length', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax2.set_xlabel('Text Length Category', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation_results/performance_by_length.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Overall confidence distribution\n",
    "axes[0, 0].hist(predictions_df['confidence'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Overall Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].axvline(predictions_df['confidence'].mean(), color='r', \n",
    "                   linestyle='--', label=f'Mean: {predictions_df[\"confidence\"].mean():.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Confidence by correctness\n",
    "correct_conf = predictions_df[predictions_df['correct']]['confidence']\n",
    "incorrect_conf = predictions_df[~predictions_df['correct']]['confidence']\n",
    "\n",
    "axes[0, 1].hist([correct_conf, incorrect_conf], bins=30, label=['Correct', 'Incorrect'],\n",
    "               alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Confidence: Correct vs Incorrect', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Accuracy by confidence bins\n",
    "conf_bins = [0, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "conf_labels = ['0.5-0.6', '0.6-0.7', '0.7-0.8', '0.8-0.9', '0.9-1.0']\n",
    "predictions_df['conf_bucket'] = pd.cut(predictions_df['confidence'], \n",
    "                                        bins=conf_bins, labels=conf_labels)\n",
    "\n",
    "conf_analysis = predictions_df.groupby('conf_bucket').agg({\n",
    "    'correct': ['count', 'mean']\n",
    "})\n",
    "conf_analysis.columns = ['count', 'accuracy']\n",
    "\n",
    "conf_analysis['accuracy'].plot(kind='bar', ax=axes[1, 0], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Accuracy by Confidence Range', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Confidence Range', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# Sample count by confidence\n",
    "conf_analysis['count'].plot(kind='bar', ax=axes[1, 1], color='orange', edgecolor='black')\n",
    "axes[1, 1].set_title('Sample Distribution by Confidence', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Confidence Range', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Number of Samples', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation_results/confidence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfidence Statistics:\")\n",
    "print(f\"Mean confidence (correct): {correct_conf.mean():.4f}\")\n",
    "print(f\"Mean confidence (incorrect): {incorrect_conf.mean():.4f}\")\n",
    "print(f\"\\nAccuracy by Confidence Range:\")\n",
    "print(conf_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Misclassification Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified examples\n",
    "errors = predictions_df[~predictions_df['correct']].copy()\n",
    "errors = errors.sort_values('confidence', ascending=False)\n",
    "\n",
    "print(f\"\\nTotal Errors: {len(errors)}\")\n",
    "print(f\"Error Rate: {len(errors)/len(predictions_df)*100:.2f}%\")\n",
    "\n",
    "# False Positives (predicted positive, actually negative)\n",
    "false_positives = errors[(errors['true_label'] == 0) & (errors['predicted_label'] == 1)]\n",
    "print(f\"\\nFalse Positives: {len(false_positives)}\")\n",
    "\n",
    "# False Negatives (predicted negative, actually positive)\n",
    "false_negatives = errors[(errors['true_label'] == 1) & (errors['predicted_label'] == 0)]\n",
    "print(f\"False Negatives: {len(false_negatives)}\")\n",
    "\n",
    "# Show hardest errors (high confidence but wrong)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HARDEST ERRORS (High Confidence but Wrong)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in errors.head(10).iterrows():\n",
    "    true_label = \"POSITIVE\" if row['true_label'] == 1 else \"NEGATIVE\"\n",
    "    pred_label = \"POSITIVE\" if row['predicted_label'] == 1 else \"NEGATIVE\"\n",
    "    \n",
    "    print(f\"\\nExample {idx+1}:\")\n",
    "    print(f\"Text: {row['text'][:200]}...\" if len(row['text']) > 200 else f\"Text: {row['text']}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted: {pred_label} (confidence: {row['confidence']:.4f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns\n",
    "print(\"\\nError Statistics:\")\n",
    "print(f\"Mean text length (errors): {errors['text_length'].mean():.0f} chars\")\n",
    "print(f\"Mean text length (correct): {predictions_df[predictions_df['correct']]['text_length'].mean():.0f} chars\")\n",
    "print(f\"\\nMean confidence (errors): {errors['confidence'].mean():.4f}\")\n",
    "print(f\"Mean confidence (correct): {predictions_df[predictions_df['correct']]['confidence'].mean():.4f}\")\n",
    "\n",
    "# Error rate by length\n",
    "error_by_length = predictions_df.groupby('length_bucket').agg({\n",
    "    'correct': lambda x: (~x).sum() / len(x) * 100\n",
    "}).round(2)\n",
    "error_by_length.columns = ['error_rate_%']\n",
    "\n",
    "print(\"\\nError Rate by Text Length:\")\n",
    "print(error_by_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations for Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "\n",
    "# Check for low accuracy\n",
    "if metrics['accuracy'] < 0.90:\n",
    "    recommendations.append(\"üî¥ Overall accuracy below 90% target - consider additional training epochs or larger dataset\")\n",
    "\n",
    "# Check for class imbalance in errors\n",
    "fp_rate = len(false_positives) / len(errors) * 100\n",
    "fn_rate = len(false_negatives) / len(errors) * 100\n",
    "if abs(fp_rate - fn_rate) > 20:\n",
    "    recommendations.append(f\"‚ö†Ô∏è  Imbalanced error types (FP: {fp_rate:.1f}%, FN: {fn_rate:.1f}%) - consider class weights\")\n",
    "\n",
    "# Check confidence calibration\n",
    "low_conf_errors = errors[errors['confidence'] < 0.7]\n",
    "if len(low_conf_errors) > len(errors) * 0.3:\n",
    "    recommendations.append(\"‚ö†Ô∏è  Many low-confidence errors - model is uncertain, consider more training data\")\n",
    "\n",
    "# Check for length bias\n",
    "short_text_acc = predictions_df[predictions_df['text_length'] < 50]['correct'].mean()\n",
    "if short_text_acc < metrics['accuracy'] - 0.05:\n",
    "    recommendations.append(f\"‚ö†Ô∏è  Poor performance on short texts ({short_text_acc:.2%}) - consider data augmentation\")\n",
    "\n",
    "# General recommendations\n",
    "recommendations.append(\"‚úÖ Analyze hardest errors above to identify failure patterns (sarcasm, negation, etc.)\")\n",
    "recommendations.append(\"‚úÖ Consider ensemble methods or model distillation for improvement\")\n",
    "recommendations.append(\"‚úÖ Collect more examples similar to frequent error cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS FOR IMPROVEMENT\")\n",
    "print(\"=\"*80)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# Save recommendations\n",
    "with open('../evaluation_results/recommendations.txt', 'w') as f:\n",
    "    f.write(\"Recommendations for Model Improvement\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        f.write(f\"{i}. {rec}\\n\")\n",
    "\n",
    "print(\"\\nRecommendations saved to evaluation_results/recommendations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This error analysis notebook has:\n",
    "1. ‚úÖ Visualized confusion matrix and ROC curve\n",
    "2. ‚úÖ Analyzed performance by text length\n",
    "3. ‚úÖ Investigated confidence score calibration\n",
    "4. ‚úÖ Identified hardest misclassifications\n",
    "5. ‚úÖ Provided actionable recommendations\n",
    "\n",
    "**Next Steps:**\n",
    "- Review hardest errors to understand failure modes\n",
    "- Implement recommended improvements\n",
    "- Re-train and re-evaluate\n",
    "- Document findings in `docs/ERROR_ANALYSIS.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
