{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Sentiment Analysis - Model Training on Google Colab\n",
    "\n",
    "This notebook trains DistilBERT and RoBERTa models for sentiment analysis.\n",
    "\n",
    "**Before running:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
    "2. Upload your `amazon_polarity_20k.csv` file (or we'll download sample data)\n",
    "\n",
    "**What this notebook does:**\n",
    "- Train DistilBERT (~90 minutes on T4 GPU)\n",
    "- Train RoBERTa (~120 minutes on T4 GPU)\n",
    "- Save both models for download\n",
    "- Generate evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Step 1: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: GPU not available! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets scikit-learn pandas tqdm accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_data"
   },
   "source": [
    "## Step 2: Upload Training Data\n",
    "\n",
    "**Option A:** Upload your `amazon_polarity_20k.csv` file using the file upload button on the left\n",
    "\n",
    "**Option B:** Use sample data from Hugging Face (we'll download it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_or_download"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if user uploaded file\n",
    "if os.path.exists('amazon_polarity_20k.csv'):\n",
    "    print(\"‚úÖ Using uploaded file: amazon_polarity_20k.csv\")\n",
    "    DATA_PATH = 'amazon_polarity_20k.csv'\n",
    "else:\n",
    "    print(\"üì• Downloading sample data from Hugging Face...\")\n",
    "    from datasets import load_dataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Download amazon_polarity dataset\n",
    "    dataset = load_dataset('amazon_polarity', split='train[:20000]')\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'full_text': dataset['content'],\n",
    "        'label': dataset['label']\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('amazon_polarity_20k.csv', index=False)\n",
    "    DATA_PATH = 'amazon_polarity_20k.csv'\n",
    "    print(f\"‚úÖ Downloaded {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_code"
   },
   "source": [
    "## Step 3: Define Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_function"
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, output_dir, epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Train a sentiment classification model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name (e.g., 'distilbert-base-uncased')\n",
    "        output_dir: Directory to save the trained model\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Training batch size\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"üìÇ Loading data...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"   Total samples: {len(df)}\")\n",
    "    \n",
    "    # Split data: 60% train, 20% val, 20% test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, random_state=42, stratify=df['label']\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=0.25, random_state=42, stratify=train_val_df['label']\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df[['full_text', 'label']].reset_index(drop=True))\n",
    "    val_dataset = Dataset.from_pandas(val_df[['full_text', 'label']].reset_index(drop=True))\n",
    "    test_dataset = Dataset.from_pandas(test_df[['full_text', 'label']].reset_index(drop=True))\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(f\"\\nü§ó Loading {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"üî§ Tokenizing...\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['full_text'],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Metrics function\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        probs = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, predictions),\n",
    "            'f1': f1_score(labels, predictions, average='weighted'),\n",
    "            'precision': precision_score(labels, predictions, average='weighted'),\n",
    "            'recall': recall_score(labels, predictions, average='weighted'),\n",
    "            'roc_auc': roc_auc_score(labels, probs[:, 1])\n",
    "        }\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/checkpoints\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,  # Use mixed precision for faster training\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nüöÄ Starting training...\\n\")\n",
    "    start_time = datetime.now()\n",
    "    trainer.train()\n",
    "    training_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nüìä Evaluating on test set...\")\n",
    "    test_results = trainer.predict(test_tokenized)\n",
    "    test_metrics = compute_metrics((test_results.predictions, test_results.label_ids))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST RESULTS - {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1-Score:  {test_metrics['f1']:.4f}\")\n",
    "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics['recall']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"\\nTraining time: {training_time:.1f} minutes\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Save model\n",
    "    print(f\"üíæ Saving model to {output_dir}/...\")\n",
    "    model.save_pretrained(output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_dict = {\n",
    "        'model_name': model_name,\n",
    "        'test_metrics': test_metrics,\n",
    "        'training_time_minutes': training_time,\n",
    "        'training_config': {\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_dir}/metrics.json\", 'w') as f:\n",
    "        json.dump(metrics_dict, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Training complete!\\n\")\n",
    "    return test_metrics, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_distilbert"
   },
   "source": [
    "## Step 4: Train DistilBERT\n",
    "\n",
    "Expected time: ~90 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "distilbert_train"
   },
   "outputs": [],
   "source": [
    "# Train DistilBERT\n",
    "distilbert_metrics, distilbert_time = train_model(\n",
    "    model_name='distilbert-base-uncased',\n",
    "    output_dir='distilbert_sentiment',\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_roberta"
   },
   "source": [
    "## Step 5: Train RoBERTa\n",
    "\n",
    "Expected time: ~120 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roberta_train"
   },
   "outputs": [],
   "source": [
    "# Train RoBERTa\n",
    "roberta_metrics, roberta_time = train_model(\n",
    "    model_name='roberta-base',\n",
    "    output_dir='roberta_sentiment',\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## Step 6: Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get model sizes\n",
    "def get_model_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "distilbert_size = get_model_size('distilbert_sentiment')\n",
    "roberta_size = get_model_size('roberta_sentiment')\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['DistilBERT', 'RoBERTa'],\n",
    "    'Accuracy': [\n",
    "        f\"{distilbert_metrics['accuracy']:.4f}\",\n",
    "        f\"{roberta_metrics['accuracy']:.4f}\"\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f\"{distilbert_metrics['f1']:.4f}\",\n",
    "        f\"{roberta_metrics['f1']:.4f}\"\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        f\"{distilbert_metrics['roc_auc']:.4f}\",\n",
    "        f\"{roberta_metrics['roc_auc']:.4f}\"\n",
    "    ],\n",
    "    'Training Time (min)': [\n",
    "        f\"{distilbert_time:.1f}\",\n",
    "        f\"{roberta_time:.1f}\"\n",
    "    ],\n",
    "    'Model Size (MB)': [\n",
    "        f\"{distilbert_size:.1f}\",\n",
    "        f\"{roberta_size:.1f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Save comparison\n",
    "comparison.to_csv('model_comparison.csv', index=False)\n",
    "print(\"üíæ Saved comparison to model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## Step 7: Download Models\n",
    "\n",
    "Zip and download the trained models to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_models"
   },
   "outputs": [],
   "source": [
    "# Zip models for download\n",
    "!zip -r distilbert_sentiment.zip distilbert_sentiment/\n",
    "!zip -r roberta_sentiment.zip roberta_sentiment/\n",
    "\n",
    "print(\"\\n‚úÖ Models zipped!\")\n",
    "print(\"\\nTo download:\")\n",
    "print(\"1. Click the folder icon on the left\")\n",
    "print(\"2. Right-click on 'distilbert_sentiment.zip' ‚Üí Download\")\n",
    "print(\"3. Right-click on 'roberta_sentiment.zip' ‚Üí Download\")\n",
    "print(\"4. Right-click on 'model_comparison.csv' ‚Üí Download\")\n",
    "\n",
    "# Also provide direct download links\n",
    "from google.colab import files\n",
    "print(\"\\nOr download directly:\")\n",
    "files.download('distilbert_sentiment.zip')\n",
    "files.download('roberta_sentiment.zip')\n",
    "files.download('model_comparison.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "After downloading:\n",
    "\n",
    "1. **Extract models** to your local `models/` directory:\n",
    "   ```bash\n",
    "   unzip distilbert_sentiment.zip -d models/\n",
    "   unzip roberta_sentiment.zip -d models/\n",
    "   ```\n",
    "\n",
    "2. **Compare with your BERT model** (91.875% accuracy)\n",
    "\n",
    "3. **Select the winner** based on:\n",
    "   - Accuracy (most important)\n",
    "   - Model size (for deployment)\n",
    "   - Inference speed (test locally)\n",
    "\n",
    "4. **Build deployment** with the best model:\n",
    "   - FastAPI REST API\n",
    "   - Gradio web demo\n",
    "   - Docker container"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
